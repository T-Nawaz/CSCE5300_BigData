{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f19e75",
   "metadata": {},
   "source": [
    "# Week 2 Assignment – Structured and Unstructured Data with Spark\n",
    "\n",
    "**Course:** CSCE 5300 – Introduction to Big Data and Data Science  \n",
    "**Topic:** Structured and Unstructured Data Processing  \n",
    "**Tools:** PySpark, JSON/CSV parsing, DataFrame transformations\n",
    "\n",
    "In this assignment, you will build upon your Spark installation from Week 1 to load and process both structured (CSV) and unstructured (JSON) data using Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac3ae72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup Spark Environment\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, to_timestamp, hour, dayofweek\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Week 2 Data Processing\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"✅ Spark session created successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to create Spark session:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da05fd1",
   "metadata": {},
   "source": [
    "## Part A – Structured Data (CSV)\n",
    "\n",
    "We'll work with the Titanic dataset to explore structured data using Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1744a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Survived: string (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- SibSp: string (nullable = true)\n",
      " |-- Parch: string (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: string (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female| 35|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male| 35|    0|    0|          373450|   8.05| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "local_path = \"data/titanic.csv\"  # Example path — change if needed\n",
    "\n",
    "df_csv = spark.read.option(\"header\", \"true\").csv(local_path)\n",
    "\n",
    "# Print schema and preview the data\n",
    "df_csv.printSchema()\n",
    "df_csv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "613c98f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+\n",
      "|   Sex|Survived|count|\n",
      "+------+--------+-----+\n",
      "|female|       0|   81|\n",
      "|female|       1|  233|\n",
      "|  male|       1|  109|\n",
      "|  male|       0|  468|\n",
      "+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop null values and count survivors by gender\n",
    "df_cleaned = df_csv.dropna(subset=[\"Survived\", \"Sex\"])\n",
    "df_cleaned.groupBy(\"Sex\", \"Survived\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033be1d8",
   "metadata": {},
   "source": [
    "## Part B – Unstructured Data (JSON)\n",
    "\n",
    "Now let’s work with Yelp Check-in data as unstructured input (semi-structured JSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef37b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n",
      "+----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|business_id           |date                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|---kPU91CF4Lq2-WlRu9Lw|2020-03-13 21:10:56, 2020-06-02 22:18:06, 2020-07-24 22:42:27, 2020-10-24 21:36:13, 2020-12-09 21:23:33, 2021-01-20 17:34:57, 2021-04-30 21:02:03, 2021-05-25 21:16:54, 2021-08-06 21:08:08, 2021-10-02 15:15:42, 2021-11-11 16:23:50                                                                                                                                                                                                                                                                                                                           |\n",
      "|--0iUa4sNDFiZFrAdIWhZQ|2010-09-13 21:43:09, 2011-05-04 23:08:15, 2011-07-18 22:30:31, 2012-09-07 20:28:50, 2013-03-27 15:57:36, 2013-08-13 00:31:34, 2013-08-13 00:31:48, 2013-09-23 17:39:38, 2013-11-18 06:34:08, 2014-04-12 23:04:47                                                                                                                                                                                                                                                                                                                                                |\n",
      "|--30_8IhuyMHbSOcNWd6DQ|2013-06-14 23:29:17, 2014-08-13 23:20:22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|--7PUidqRWpRSpXebiyxTg|2011-02-15 17:12:00, 2011-07-28 02:46:10, 2012-03-11 10:30:02, 2012-04-24 07:07:59, 2012-04-24 07:43:31, 2013-05-25 16:41:10, 2014-05-02 15:49:55, 2014-09-18 02:28:23, 2014-11-10 15:16:43, 2015-09-27 13:18:32                                                                                                                                                                                                                                                                                                                                                |\n",
      "|--7jw19RH9JKXgFohspgQw|2014-04-21 20:42:11, 2014-04-28 21:04:46, 2014-09-30 14:41:47, 2014-10-23 18:22:28, 2015-04-27 19:55:00, 2015-09-21 12:52:09, 2015-10-01 12:46:16, 2015-10-22 13:35:04, 2016-01-14 12:27:43, 2016-02-01 15:15:07, 2016-02-11 12:22:47, 2016-03-31 23:15:46, 2016-04-11 13:11:34, 2016-05-25 12:44:11, 2016-06-27 15:46:11, 2016-06-30 15:14:28, 2016-07-28 14:15:59, 2016-09-22 20:05:06, 2016-11-16 19:00:11, 2016-12-27 14:36:14, 2017-01-24 20:18:16, 2017-02-21 16:01:49, 2017-03-21 14:08:44, 2017-05-21 16:07:21, 2017-08-07 14:17:39, 2021-06-21 19:59:50|\n",
      "+----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Load Yelp Check-in JSON (place your file in the same folder or update the path)\n",
    "df_json = spark.read.json(\"data/yelp-dataset/yelp_academic_dataset_checkin.json\")\n",
    "df_json.printSchema()\n",
    "df_json.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "440ca7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|         business_id|       checkin_time|\n",
      "+--------------------+-------------------+\n",
      "|---kPU91CF4Lq2-Wl...|2020-03-13 21:10:56|\n",
      "|---kPU91CF4Lq2-Wl...|2020-06-02 22:18:06|\n",
      "|---kPU91CF4Lq2-Wl...|2020-07-24 22:42:27|\n",
      "|---kPU91CF4Lq2-Wl...|2020-10-24 21:36:13|\n",
      "|---kPU91CF4Lq2-Wl...|2020-12-09 21:23:33|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Split and transform the date field\n",
    "df_split = df_json.withColumn(\"checkin_time\", explode(split(df_json[\"date\"], \", \")))\n",
    "df_clean = df_split.withColumn(\"checkin_time\", to_timestamp(\"checkin_time\"))\n",
    "df_clean.select(\"business_id\", \"checkin_time\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "996e6fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|hour|  count|\n",
      "+----+-------+\n",
      "|   0|1155092|\n",
      "|   1| 935985|\n",
      "|   2| 665907|\n",
      "|   3| 440702|\n",
      "|   4| 264905|\n",
      "|   5| 152476|\n",
      "|   6|  85066|\n",
      "|   7|  52295|\n",
      "|   8|  35589|\n",
      "|   9|  37079|\n",
      "|  10|  63824|\n",
      "|  11| 115876|\n",
      "|  12| 201427|\n",
      "|  13| 296364|\n",
      "|  14| 407969|\n",
      "|  15| 587904|\n",
      "|  16| 873108|\n",
      "|  17|1018438|\n",
      "|  18| 995358|\n",
      "|  19| 922177|\n",
      "+----+-------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:==============>                                          (3 + 9) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|day_of_week|  count|\n",
      "+-----------+-------+\n",
      "|          1|2480701|\n",
      "|          2|1491993|\n",
      "|          3|1460432|\n",
      "|          4|1541769|\n",
      "|          5|1612496|\n",
      "|          6|1959015|\n",
      "|          7|2810469|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Extract time-based features and aggregate\n",
    "df_features = df_clean.withColumn(\"hour\", hour(\"checkin_time\")) \\\n",
    "                      .withColumn(\"day_of_week\", dayofweek(\"checkin_time\"))\n",
    "df_features.groupBy(\"hour\").count().orderBy(\"hour\").show()\n",
    "df_features.groupBy(\"day_of_week\").count().orderBy(\"day_of_week\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6827017",
   "metadata": {},
   "source": [
    "## Part C – Spark Exploration Challenge 🔍\n",
    "\n",
    "In this section, you will complete the following tasks by filling in the missing parts of the provided code:\n",
    "\n",
    "### 🎯 Your Task:\n",
    "- Show the **last 5 rows** of the Titanic dataset using Spark.\n",
    "- Select only important columns such as: `PassengerId`, `Name`, `Survived`, `Pclass`, `Sex`, `Age`\n",
    "- (Optional Bonus) Sort by `Age` in descending order and show the top 5 oldest passengers.\n",
    "\n",
    "### 🧩 Starter Code:\n",
    "Run the following cells and **fill in the blanks** where indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eb0cf89-4b10-435a-9be1-3e6341a0db40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+-------+-------+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|  Ticket|   Fare|  Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+-------+-------+--------+\n",
      "|         99|       1|     2|Doling, Mrs. John...|female|  34|    0|    1|  231919|     23|   NULL|       S|\n",
      "|         98|       1|     1|Greenfield, Mr. W...|  male|  23|    0|    1|PC 17759|63.3583|D10 D12|       C|\n",
      "|         97|       0|     1|Goldschmidt, Mr. ...|  male|  71|    0|    0|PC 17754|34.6542|     A5|       C|\n",
      "|         96|       0|     3|Shorney, Mr. Char...|  male|NULL|    0|    0|  374910|   8.05|   NULL|       S|\n",
      "|         95|       0|     3|   Coxon, Mr. Daniel|  male|  59|    0|    0|  364500|   7.25|   NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+-------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show last 5 rows (Hint: sort by a column and use .tail equivalent)\n",
    "# Replace '___' with the correct DataFrame methods\n",
    "\n",
    "df_csv.orderBy(\"PassengerId\", ascending=False).limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98cf9e51-7458-494c-add5-860478bbda1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------+------+------+---+\n",
      "|PassengerId|                Name|Survived|Pclass|   Sex|Age|\n",
      "+-----------+--------------------+--------+------+------+---+\n",
      "|          1|Braund, Mr. Owen ...|       0|     3|  male| 22|\n",
      "|          2|Cumings, Mrs. Joh...|       1|     1|female| 38|\n",
      "|          3|Heikkinen, Miss. ...|       1|     3|female| 26|\n",
      "|          4|Futrelle, Mrs. Ja...|       1|     1|female| 35|\n",
      "|          5|Allen, Mr. Willia...|       0|     3|  male| 35|\n",
      "+-----------+--------------------+--------+------+------+---+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Select important columns only (fill in the list)\n",
    "columns_to_select = [\"PassengerId\", \"Name\" , \"Survived\", \"Pclass\", \"Sex\" , \"Age\"]\n",
    "\n",
    "df_csv.select(columns_to_select).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3842382-3f06-45e4-b2c5-b45202b79071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|                Name| Age|\n",
      "+--------------------+----+\n",
      "|Barkworth, Mr. Al...|80.0|\n",
      "| Svensson, Mr. Johan|74.0|\n",
      "|Artagaveytia, Mr....|71.0|\n",
      "|Goldschmidt, Mr. ...|71.0|\n",
      "|Connors, Mr. Patrick|70.5|\n",
      "+--------------------+----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# BONUS: Show top 5 oldest passengers\n",
    "df_csv.selectExpr(\"Name\", \"cast(Age as double) as Age\").orderBy(\"Age\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208170b5-264a-488f-93f0-e5f6eeae0aa8",
   "metadata": {},
   "source": [
    "### 🎯 Task 2: Yelp JSON\n",
    "- Split the `date` field into individual timestamps\n",
    "- Extract the **hour** from check-in times\n",
    "- Group and count check-ins by hour\n",
    "\n",
    "🔧 Fill in the blanks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bc61c9e-1e89-4f4d-8494-99b4fac2a51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:======================================>                  (8 + 4) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|hour|  count|\n",
      "+----+-------+\n",
      "|   0|1155092|\n",
      "|   1| 935985|\n",
      "|   2| 665907|\n",
      "|   3| 440702|\n",
      "|   4| 264905|\n",
      "|   5| 152476|\n",
      "|   6|  85066|\n",
      "|   7|  52295|\n",
      "|   8|  35589|\n",
      "|   9|  37079|\n",
      "|  10|  63824|\n",
      "|  11| 115876|\n",
      "|  12| 201427|\n",
      "|  13| 296364|\n",
      "|  14| 407969|\n",
      "|  15| 587904|\n",
      "|  16| 873108|\n",
      "|  17|1018438|\n",
      "|  18| 995358|\n",
      "|  19| 922177|\n",
      "+----+-------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Load JSON data (already done in earlier cell)\n",
    "df_json = spark.read.json(\"data/yelp-dataset/yelp_academic_dataset_checkin.json\")\n",
    "\n",
    "# Transform the 'date' column into individual timestamps\n",
    "from pyspark.sql.functions import split, explode, to_timestamp, hour\n",
    "\n",
    "df_split = df_json.withColumn(\"checkin_time\", explode(split(df_json[\"date\"], \", \")))\n",
    "\n",
    "# Convert to timestamp\n",
    "df_clean = df_split.withColumn(\"checkin_time\", to_timestamp(\"checkin_time\"))\n",
    "\n",
    "# Extract hour and group\n",
    "df_hour = df_clean.withColumn(\"hour\", hour(\"checkin_time\"))\n",
    "df_hour.groupBy(\"hour\").count().orderBy(\"hour\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "039c912b-2feb-4e46-8038-57cc30c11c53",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, date: string, checkin_time: timestamp]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c0db97c-4838-4e65-a8d2-1762424575df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>                                                       (0 + 12) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|         business_id|count|\n",
      "+--------------------+-----+\n",
      "|-QI8Qi8XWH3D8y8et...|52144|\n",
      "|FEXhWNCMkv22qG04E...|40109|\n",
      "|Eb1XmmLWyt_way5NN...|37562|\n",
      "|c_4c5rJECZSfNgFj7...|37518|\n",
      "|4i4kmYm9wgSNyF1b6...|31168|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Group check-ins by business and count how many check-ins each business has\n",
    "df_clean.groupBy(\"business_id\").count().orderBy(\"count\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c9d31d1-081e-45d7-b148-aa09e3351b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:>                                                       (0 + 12) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|         business_id|     latest_checkin|\n",
      "+--------------------+-------------------+\n",
      "|A2WOHcAk-BkMyf9Qg...|2022-01-19 16:48:37|\n",
      "|CjNjiHsyV-gvtOsHZ...|2022-01-19 16:46:55|\n",
      "|9m7mHDbSLIUjFwuuV...|2022-01-19 16:45:11|\n",
      "|6R-hg8Ee51PDQsjaP...|2022-01-19 16:44:26|\n",
      "|kZu9sbMkvbpDCpqmm...|2022-01-19 16:43:59|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "# Find the most recent check-in time per business\n",
    "df_clean.groupBy(\"business_id\").agg(max(\"checkin_time\").alias(\"latest_checkin\")).orderBy(\"latest_checkin\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64d4563-88e7-4af1-86f7-69df162f30f6",
   "metadata": {},
   "source": [
    "## Part D – Reflection ✍️\n",
    "\n",
    "**Write a short reflection (2–5 sentences):**\n",
    "- What differences did you observe between handling CSV and JSON in Spark?\n",
    "- What was easier or harder, and why?\n",
    "\n",
    "_You can write this in the Markdown cell below or submit as a separate note._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f0d079",
   "metadata": {},
   "source": [
    "### CSV vs JSON in Spark\n",
    "\n",
    "I have previous experience with handling CSV files with pandas and so the data processing for CSV file with Spark felt familiar to me. Operations like groupBy(), count() were almost identical and operations like select(), orderBy() had familiarity to them. CSV files are well structured by design and that made it easier to manipulate and analyze.\n",
    "\n",
    "The JSON data processing on the other hand felt a bit more tricky, owning to is semi-structured format. The JSON data needed some specific operations like explode() and split() to convert into individual records before I could start with the analysis steps. To me, it felt that I needed to put in more effort and needed to have deeper understanding of JSON structure to be able to even start working with this type of data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
